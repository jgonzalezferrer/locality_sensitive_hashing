{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Javier González Ferrer & Hamed Mohammadpour - KTH Royal Institute of Technology 2017-11-12 \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.1.0\n",
      "Git repo: git@github.com:jgonzalezferrer/locality_sensitive_hashing.git\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a 'Javier González Ferrer & Hamed Mohammadpour - KTH Royal Institute of Technology' -v -d -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Sensitive Hashing Demo\n",
    "\n",
    "In this notebook we are going to see our implementation of LSH step by step. \n",
    "\n",
    "First we are going to load the data and then run and visualize:\n",
    "- Shingling class\n",
    "- MinHash class\n",
    "- LSH class\n",
    "\n",
    "Let's get started by importing all the libraries in the first cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locality_sensitive_hashing.utility as utility\n",
    "import locality_sensitive_hashing.shingling as shingling\n",
    "import locality_sensitive_hashing.minhashing as minhashing\n",
    "import locality_sensitive_hashing.lsh as lsh\n",
    "\n",
    "from tqdm import tqdm, trange, tqdm_notebook # Printing progress bar\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For printing maps and dictionaries in sorted, beautiful format\n",
    "def printify(my_dict):\n",
    "    print(json.dumps(my_dict, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The following data is acquired from [here](http://www.inf.ed.ac.uk/teaching/courses/tts/assessed/assessment3.html)\n",
    "\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can run this code for different portions of the dataset.\n",
    "# It ships with data set sizes 100, 1000, 2500, and 10000.\n",
    "numDocs = 100\n",
    "dataFile = \"./data/articles_\" + str(numDocs) + \".train\"\n",
    "truthFile = \"./data/articles_\" + str(numDocs) + \".truth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parse The Ground Truth Tables:\n",
    "Build a dictionary mapping the document IDs to their plagiaries, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"t1088\": \"t5015\",\n",
      " \"t5015\": \"t1088\",\n",
      " \"t1297\": \"t4638\",\n",
      " \"t4638\": \"t1297\",\n",
      " \"t1768\": \"t5248\",\n",
      " \"t5248\": \"t1768\",\n",
      " \"t1952\": \"t3495\",\n",
      " \"t3495\": \"t1952\",\n",
      " \"t980\": \"t2023\",\n",
      " \"t2023\": \"t980\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "plagiaries = {}\n",
    "\n",
    "# Open the truth file.\n",
    "f = open(truthFile, \"r\")\n",
    "\n",
    "# For each line of the files...\n",
    "for line in f:\n",
    "  \n",
    "    # Strip the newline character, if present.\n",
    "    if line[-1] == '\\n':\n",
    "        line = line[0:-1]\n",
    "\n",
    "    docs = line.split(\" \")\n",
    "\n",
    "    # Map the two documents to each other.\n",
    "    plagiaries[docs[0]] = docs[1]\n",
    "    plagiaries[docs[1]] = docs[0]\n",
    "\n",
    "# Close the data file.  \n",
    "f.close()  \n",
    "\n",
    "printify(plagiaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute k-shingles of all documents we have\n",
    "\n",
    "While creating of k-shingles, we also use a hash functions to convert them to a 4-bit long integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77d428d1d2c4cdb9e5afecc7605732e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 303 ms, sys: 34.9 ms, total: 338 ms\n",
      "Wall time: 481 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 5 # k-shingle hyper-parameter\n",
    "\n",
    "doc_list = []\n",
    "shingle_set = {}\n",
    "\n",
    "f = open(dataFile, \"r\")\n",
    "\n",
    "for i in tqdm_notebook(range(numDocs)):\n",
    "    document = f.readline()\n",
    "    doc_id, doc_body = document.split(\" \", 1)\n",
    "    \n",
    "    doc_list.append(doc_id)\n",
    "    shingle_set[doc_id] = shingling.Shingling(doc_body, k).shingles\n",
    "    \n",
    "# Close the data file.  \n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute Jaccard similarity \n",
    "In this section we will compute Jaccard similarity with the utility function for later comparison with MinHash and LSH accuracy. \n",
    "\n",
    "[Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) is defined as: \n",
    "$$ J(A,B)={{|A\\cap B|} \\over {|A\\cup B|}}={{|A\\cap B|} \\over {|A|+|B|-|A\\cap B|}}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1be0081f334147a19260535298272a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents t980 and t2023 have jaccard sim. of 0.9901\n",
      "Documents t1088 and t5015 have jaccard sim. of 0.9916\n",
      "Documents t1297 and t4638 have jaccard sim. of 0.9902\n",
      "Documents t1768 and t5248 have jaccard sim. of 0.9901\n",
      "Documents t1952 and t3495 have jaccard sim. of 0.9869\n",
      "\n",
      "{\n",
      " \"t980\": \"t2023\",\n",
      " \"t2023\": \"t980\",\n",
      " \"t1088\": \"t5015\",\n",
      " \"t5015\": \"t1088\",\n",
      " \"t1297\": \"t4638\",\n",
      " \"t4638\": \"t1297\",\n",
      " \"t1768\": \"t5248\",\n",
      " \"t5248\": \"t1768\",\n",
      " \"t1952\": \"t3495\",\n",
      " \"t3495\": \"t1952\"\n",
      "}\n",
      "CPU times: user 1.05 s, sys: 73.2 ms, total: 1.12 s\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# run test if the number of docs is small\n",
    "jaccard_threshhold = 0.60\n",
    "similar_docs = {}\n",
    "if numDocs <= 2500:\n",
    "    \n",
    "    for i in tqdm_notebook(range(numDocs), desc='1st loop'):\n",
    "        set1 = shingle_set[doc_list[i]]\n",
    "        \n",
    "        for j in range(i+1, numDocs):\n",
    "            set2 = shingle_set[doc_list[j]]\n",
    "            jaccard_sim = utility.compare_sets(set1, set2)\n",
    "            if jaccard_sim >= jaccard_threshhold:\n",
    "                similar_docs[doc_list[i]] = doc_list[j]\n",
    "                similar_docs[doc_list[j]] = doc_list[i]\n",
    "                print(\"Documents {} and {} have jaccard sim. of {:.4f}\"\n",
    "                      .format(doc_list[i], doc_list[j], jaccard_sim))\n",
    "\n",
    "printify(similar_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHash\n",
    "\n",
    "Comparing all possible combinations of k-shingles of documents for large number of documents can take a very long time or become totally incomputable. For this reason we use MinHash algorithm which creates a unique hash of fixed lenght (from number of hash functions n) so all documents get a signiture of length n. \n",
    "\n",
    "\n",
    "### 4.1 Compute MinHash of documents\n",
    "\n",
    "#### 4.1.1 Finding hash functions\n",
    "For generating `n` random hash functions, we used the following method inspired from [here](https://en.wikipedia.org/wiki/Universal_hashing#Hashing_integers)\n",
    "\n",
    "$$ h_{a,b}(x)=((ax+b)~{\\bmod {~}}p)~{\\bmod {~}}m $$\n",
    "\n",
    "where $a$ and $b$ are random numbers between $1$ and $ m = 2 ^{32} - 1$, $c$ is a prime number larger than m which is maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4be6186071c402391407b4f06ba0e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 8.88 s, sys: 280 ms, total: 9.16 s\n",
      "Wall time: 9.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n = 100 # Number of minhash hash functions\n",
    "minhash_doc_list = []\n",
    "\n",
    "for i in tqdm_notebook(range(numDocs), \n",
    "                       desc=\"Calculating min hash for {} documents\".format(numDocs)):\n",
    "        set1 = shingle_set[doc_list[i]]\n",
    "        min1 = minhashing.MinHashing(set1, n)\n",
    "        minhash_doc_list.append(min1.signature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Compare all document pairs for finding duplicates\n",
    "In this section we will compare the minhash of documnts which we calculated in previous part to find the similar documents. This comparison takes less than 80 ms for 100 documents where as Jaccard similarity took around 9s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a62c714ed554c0ab1d632e7414760d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents t980 and t2023 have minhash sim. of 1.0000\n",
      "Documents t1088 and t5015 have minhash sim. of 0.9900\n",
      "Documents t1297 and t4638 have minhash sim. of 1.0000\n",
      "Documents t1768 and t5248 have minhash sim. of 0.9900\n",
      "Documents t1952 and t3495 have minhash sim. of 1.0000\n",
      "\n",
      "CPU times: user 70.2 ms, sys: 14.1 ms, total: 84.3 ms\n",
      "Wall time: 79.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "minhash_threshhold = jaccard_threshhold\n",
    "\n",
    "for i in tqdm_notebook(range(numDocs), desc='1st loop'):\n",
    "        min1 = minhash_doc_list[i]\n",
    "        \n",
    "        for j in range(i+1, numDocs):\n",
    "            min2 = minhash_doc_list[j]\n",
    "            \n",
    "            minhash_sim = utility.compare_signatures(min1, min2)\n",
    "            if minhash_sim >= minhash_threshhold:\n",
    "                print(\"Documents {} and {} have minhash sim. of {:.4f}\"\n",
    "                      .format(doc_list[i], doc_list[j], minhash_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH\n",
    "\n",
    "So we arrive to the fun part of the assigment, to find similar documents using Locality Sensitive Hashing. \n",
    "\n",
    "For this, we will pass a dictionary of Document ids and a list of minhash signiture for each document. \n",
    "In the LSH class, we calculate the best number of `r` and `b` by iterating through factors of `n` which is number of signitures and choose the ones which appriximate closer to $ t = (1/b) ^{(1/r)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b is 10 and r is 10, And best approximation for threshold 0.8 is t = 0.7943282347242815\n",
      "10 10\n",
      "CPU times: user 4.72 ms, sys: 1.73 ms, total: 6.45 ms\n",
      "Wall time: 5.47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lsh_threshold = 0.8\n",
    "doc_signitures_dict = dict(zip(doc_list, minhash_doc_list))\n",
    "\n",
    "lsh_sim = lsh.LSH(doc_signitures_dict, lsh_threshold).similar_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t1088': 't5015',\n",
       " 't1297': 't4638',\n",
       " 't1768': 't5248',\n",
       " 't1952': 't3495',\n",
       " 't2023': 't980',\n",
       " 't3495': 't1952',\n",
       " 't4638': 't1297',\n",
       " 't5015': 't1088',\n",
       " 't5248': 't1768',\n",
       " 't980': 't2023'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsh_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see it can find similar documents among 100 document in less than 6 ms in comparison to 80 ms of minhash algorithm and ~9s of using raw Jaccard similary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going with larger documents\n",
    "\n",
    "Now we are ready to test our implementation on a bigger sets. You can set `numDocs` to larger number to see the performance. In our tests with 10,000 documents, the minhashing process took around 16 min. Then comparing the signituares one by one would take around 20 minutes while using LSH this time reduces to 1.14 seconds.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final thoughts\n",
    "\n",
    "In this notebook we demonstrated the performance and API of our implementation of Local Sensitive Hashing for finding similar documents. \n",
    "\n",
    "1. Surprising enough, we got 100% accuracy even testing with 10,000 documents. \n",
    "2. For calculating number of bands and rows for `LSH`, we iterate over factors of n to find best `b` and `r` to approximate as much as we can.\n",
    "\n",
    "Our code among with report and data files are published in the following repository on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Javier González Ferrer & Hamed Mohammadpour 2017-11-12 \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.1.0\n",
      "Git repo: git@github.com:jgonzalezferrer/locality_sensitive_hashing.git\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a 'Javier González Ferrer & Hamed Mohammadpour' -v -d -r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_metadata": {
   "affiliation": "KTH Royal Institute of Technology",
   "author": "Javier González Ferrer, Hamed Mohammadpour",
   "title": "Lab 1 - finding similar items report"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
