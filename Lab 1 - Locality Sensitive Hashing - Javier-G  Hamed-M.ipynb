{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonio Javier González Ferrer & Hamed Mohammadpour - KTH Royal Institute of Technology 2017-11-12 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 6.2.1\n",
      "Git repo: https://github.com/jgonzalezferrer/locality_sensitive_hashing.git\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a 'Antonio Javier González Ferrer & Hamed Mohammadpour - KTH Royal Institute of Technology' -v -d -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we are going to see the different stages of finding textually similar documents based on Jaccard similarity using the shingling, minhashing, and locality-sensitive hashing (LSH) techniques and corresponding algorithms.\n",
    "\n",
    "First we are going to load the data and then run and visualize the following classes:\n",
    "- Shingling \n",
    "- MinHash \n",
    "- LSH \n",
    "\n",
    "Let's get started by importing all the necessary libraries in the first cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from locality_sensitive_hashing.utility import compare_sets, compare_signatures\n",
    "from locality_sensitive_hashing.shingling import Shingling\n",
    "from locality_sensitive_hashing.minhashing import MinHashing\n",
    "from locality_sensitive_hashing.lsh import LSH\n",
    "\n",
    "from tqdm import tqdm, trange, tqdm_notebook # Printing progress bar\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For printing maps and dictionaries in sorted, beautiful format\n",
    "def printify(my_dict):\n",
    "    print(json.dumps(my_dict, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The [data](http://www.inf.ed.ac.uk/teaching/courses/tts/assessed/assessment3.html) we have used to test the algorithms contains piece of news where the goal is to detect plagiarism between them and has been extracted from the School of Informatics of The University of Edimburgh.\n",
    "\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run this code for different portions of the dataset.\n",
    "# It ships with data set sizes 100, 1000, 2500, and 10000.\n",
    "num_docs = 100\n",
    "data_file = \"./data/articles_\" + str(num_docs) + \".train\"\n",
    "truth_file = \"./data/articles_\" + str(num_docs) + \".truth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parse The Ground Truth Tables\n",
    "We first build a dictionary mapping the document IDs to their plagiaries, and vice versa for testing the performance of the algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"t1088\": \"t5015\",\n",
      " \"t1952\": \"t3495\",\n",
      " \"t1297\": \"t4638\",\n",
      " \"t4638\": \"t1297\",\n",
      " \"t1768\": \"t5248\",\n",
      " \"t3495\": \"t1952\",\n",
      " \"t2023\": \"t980\",\n",
      " \"t5248\": \"t1768\",\n",
      " \"t5015\": \"t1088\",\n",
      " \"t980\": \"t2023\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "plagiaries = {}\n",
    "\n",
    "# Open the truth file.\n",
    "with open(truth_file, \"r\") as f:\n",
    "    for line in f:\n",
    "\n",
    "        # Strip the newline character, if present.\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[0:-1]\n",
    "\n",
    "        docs = line.split(\" \")\n",
    "\n",
    "        # Map the two documents to each other.\n",
    "        plagiaries[docs[0]] = docs[1]\n",
    "        plagiaries[docs[1]] = docs[0]\n",
    "\n",
    "printify(plagiaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute k-shingles of all documents\n",
    "\n",
    "The first approach is to convert the documents into sets. We split the words of each document into $k$ different shingles. In this case, a $k$-shingle is a sequence of $k$ consecutive characters that appears within the documents. We assume we do not have repeated shingles into one document. \n",
    "\n",
    "The value of $k$ should be large enough specially for large documents. We have set this value to $k = 9$. Furthermore, while creating of $k$-shingles, we use a hash function to compress them to a $4$-byte integer. Therefore, the range of shingles will be from $0$ up to $2^{32}-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 208 ms, sys: 4 ms, total: 212 ms\n",
      "Wall time: 213 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 9 # k-shingle hyper-parameter\n",
    "\n",
    "doc_list = []\n",
    "shingle_set = {}\n",
    "\n",
    "with open(data_file, \"r\") as f:\n",
    "\n",
    "    for i in tqdm_notebook(range(num_docs)):\n",
    "        document = f.readline()\n",
    "        doc_id, doc_body = document.split(\" \", 1)\n",
    "\n",
    "        doc_list.append(doc_id)\n",
    "        shingle_set[doc_id] = Shingling(doc_body, k).shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute Jaccard similarity \n",
    "In this section we will compute Jaccard similarity for shingles with the utility function for later comparison with MinHash and LSH accuracy. The [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) is defined as: \n",
    "$$ J(A,B)={{|A\\cap B|} \\over {|A\\cup B|}}={{|A\\cap B|} \\over {|A|+|B|-|A\\cap B|}}. $$\n",
    "\n",
    "Therefore, two documents $A$ and $B$ will be similar is they have common shingles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 100 documents -> 4950 comparisons...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents t980 and t2023 have jaccard sim. of 0.9840\n",
      "Documents t1088 and t5015 have jaccard sim. of 0.9870\n",
      "Documents t1297 and t4638 have jaccard sim. of 0.9850\n",
      "Documents t1768 and t5248 have jaccard sim. of 0.9857\n",
      "Documents t1952 and t3495 have jaccard sim. of 0.9826\n",
      "\n",
      "{\n",
      " \"t980\": \"t2023\",\n",
      " \"t1952\": \"t3495\",\n",
      " \"t1297\": \"t4638\",\n",
      " \"t4638\": \"t1297\",\n",
      " \"t1768\": \"t5248\",\n",
      " \"t3495\": \"t1952\",\n",
      " \"t2023\": \"t980\",\n",
      " \"t5248\": \"t1768\",\n",
      " \"t5015\": \"t1088\",\n",
      " \"t1088\": \"t5015\"\n",
      "}\n",
      "CPU times: user 896 ms, sys: 12 ms, total: 908 ms\n",
      "Wall time: 904 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Run test if the number of docs is small\n",
    "jaccard_threshhold = 0.60\n",
    "similar_docs = {}\n",
    "if num_docs <= 2500:\n",
    "    print(\"Comparing {} documents -> {} comparisons...\".format(num_docs, num_docs*(num_docs-1)//2))\n",
    "    \n",
    "    for i in tqdm_notebook(range(num_docs), desc='1st loop'):\n",
    "        shingle_i = shingle_set[doc_list[i]]\n",
    "        \n",
    "        for j in range(i+1, num_docs):\n",
    "            shingle_j = shingle_set[doc_list[j]]\n",
    "            jaccard_sim = compare_sets(shingle_i, shingle_j)\n",
    "            if jaccard_sim >= jaccard_threshhold:\n",
    "                similar_docs[doc_list[i]] = doc_list[j]\n",
    "                similar_docs[doc_list[j]] = doc_list[i]\n",
    "                print(\"Documents {} and {} have jaccard sim. of {:.4f}\"\n",
    "                      .format(doc_list[i], doc_list[j], jaccard_sim))\n",
    "\n",
    "printify(similar_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHash\n",
    "\n",
    "Comparing all possible combinations of $k$-shingles of documents for large number of documents can take a very long time or become totally incomputable. For this reason we use MinHash algorithm which creates a unique hash of fixed length (from a number of hash functions $n$) so all documents get a signature of length $n$. \n",
    "\n",
    "\n",
    "### 4.1 Compute MinHash of documents\n",
    "\n",
    "#### 4.1.1 Finding hash functions\n",
    "For generating $n$ random hash functions, we used the universal hashing method inspired from [here](https://en.wikipedia.org/wiki/Universal_hashing#Hashing_integers):\n",
    "\n",
    "$$ h_{a,b}(x)=((ax+b)~{\\bmod {~}}p)~{\\bmod {~}}m $$\n",
    "\n",
    "where $a$ and $b$ are random numbers between $1$ and $ m = 2 ^{32} - 1$, $c$ is a prime number larger than $m$, and $m$ is the maximum possible value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Calculating min hash for 100 documents'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.88 s, sys: 4 ms, total: 6.89 s\n",
      "Wall time: 6.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n = 100 # Number of minhash hash functions\n",
    "minhash_doc_list = []\n",
    "\n",
    "for i in tqdm_notebook(range(num_docs), \n",
    "                       desc=\"Calculating min hash for {} documents\".format(num_docs)):\n",
    "        shingles_i = shingle_set[doc_list[i]]\n",
    "        minhashing_i = MinHashing(shingles_i, n)\n",
    "        minhash_doc_list.append(minhashing_i.signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the signatures takes some time but after you have calculated once you don't need to calculate it again. Now we have reduced large sets to short signatures, while preserving similarity. The idea behind MinHashing is that given two documents $S_1$, $S_2$, then $Pr[h_{min}(S_1) = h_{min}(S_2)] = J(S_1, S_2)$. Therefore, the expeced similarity of two signatures is equal to the Jaccard similarity of the two documents. The more hash functions (the longer the signatures) we use, the smaller will be the expected error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Compare all document pairs for finding duplicates\n",
    "In this section we will compare the minhash of documents which we calculated in previous part to find the similar documents. This comparison takes less than 60ms for 100 documents where as Jaccard similarity took around 8000ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='1st loop'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents t980 and t2023 have minhash sim. of 0.9900\n",
      "Documents t1088 and t5015 have minhash sim. of 1.0000\n",
      "Documents t1297 and t4638 have minhash sim. of 0.9700\n",
      "Documents t1768 and t5248 have minhash sim. of 0.9800\n",
      "Documents t1952 and t3495 have minhash sim. of 0.9900\n",
      "\n",
      "CPU times: user 56 ms, sys: 4 ms, total: 60 ms\n",
      "Wall time: 59.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "minhash_threshhold = jaccard_threshhold\n",
    "\n",
    "for i in tqdm_notebook(range(num_docs), desc='1st loop'):\n",
    "        minhash_i = minhash_doc_list[i]\n",
    "        \n",
    "        for j in range(i+1, num_docs):\n",
    "            minhash_j = minhash_doc_list[j]\n",
    "            \n",
    "            minhash_sim = compare_signatures(minhash_i, minhash_j)\n",
    "            if minhash_sim >= minhash_threshhold:\n",
    "                print(\"Documents {} and {} have minhash sim. of {:.4f}\"\n",
    "                      .format(doc_list[i], doc_list[j], minhash_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH\n",
    "\n",
    "However, we still have the problem of scalability and the large number of comparisons. The Locality Sensitive Hashing (LSH) solves this problem by generating pairs of candidate documents which a large likelihood of being similar. To do this, we will pass a dictionary of document ids and a list of minhash signature for each document. Given a similarity threshold $t$, the algorithm will divide this signature matrix $M$ into $b$ bands and $r$ rows and will hash columns of $M$ with the idea of arranging similar columns to same buckets. The key thing here is to choose the best $r$ and $b$ values in order to catch most similar pairs, but few non-similar ones. \n",
    "\n",
    "In the LSH class, we calculate the best number of $r$ and $b$ by iterating through the factors of $n$ which is the number of signatures and choose the ones which are approximate closer to $ t = (1/b) ^{(1/r)} $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b is 10 and r is 10, And best approximation for threshold 0.8 is t = 0.7943282347242815\n",
      "10 10\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 4.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lsh_threshold = 0.8\n",
    "doc_signatures_dict = dict(zip(doc_list, minhash_doc_list))\n",
    "\n",
    "lsh_sim = LSH(doc_signatures_dict, lsh_threshold).similar_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t1088': 't5015',\n",
       " 't1297': 't4638',\n",
       " 't1768': 't5248',\n",
       " 't1952': 't3495',\n",
       " 't2023': 't980',\n",
       " 't3495': 't1952',\n",
       " 't4638': 't1297',\n",
       " 't5015': 't1088',\n",
       " 't5248': 't1768',\n",
       " 't980': 't2023'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsh_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see it can find similar documents among 100 document in less than 4.2 ms in comparison to 80 ms of minhash algorithm and ~8000ms using raw Jaccard similary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going with larger documents\n",
    "\n",
    "Now we are ready to test our implementation on a bigger sets. You can set `num_docs` to larger number to see the performance. In our tests with 10,000 documents, the minhashing process took around 16 min. Then comparing the signatuares one by one would take around 20 minutes while using LSH this time reduces to 1.14 seconds.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final thoughts\n",
    "\n",
    "In this notebook we demonstrated the performance and API of our implementation of Local Sensitive Hashing for finding similar documents. \n",
    "\n",
    "1. Surprising enough, the LSH method reports 100% accuracy even testing with 10,000 documents. \n",
    "2. For calculating number of bands and rows for `LSH`, we iterate over factors of n to find best `b` and `r` to approximate as much as we can.\n",
    "\n",
    "Our code among with report and data files are published in the following repository on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonio Javier González Ferrer & Hamed Mohammadpour 2017-11-12 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 6.2.1\n",
      "Git repo: https://github.com/jgonzalezferrer/locality_sensitive_hashing.git\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a 'Antonio Javier González Ferrer & Hamed Mohammadpour' -v -d -r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_metadata": {
   "affiliation": "KTH Royal Institute of Technology",
   "author": "Javier González Ferrer, Hamed Mohammadpour",
   "title": "Lab 1 - finding similar items report"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}